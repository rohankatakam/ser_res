---
name: Rec Engine MVP Completion
overview: Complete the Serafis "For You" recommendation engine MVP through algorithm tuning (v1.4), infrastructure deployment (Qdrant + LiteLLM + Multi-LLM Judge), and final documentation. Target 7/7 tests passing with maximized scores.
todos:
  - id: phase4-create-v1_4
    content: Clone v1_3_tuned to v1_4_optimized, update manifest.json and config.json
    status: pending
  - id: phase4-tune-bookmark-a
    content: Try bookmark_weight 7.0, run full test suite with LLM
    status: pending
  - id: phase4-tune-bookmark-b
    content: If needed, try bookmark_weight 10.0, run full test suite
    status: pending
  - id: phase4-category-boost
    content: If still failing, experiment with category_boost parameter for bookmarked topics
    status: pending
  - id: phase4-maximize-scores
    content: Optimize all criteria scores beyond pass/fail, document improvements
    status: pending
  - id: phase4-verify-7-7
    content: Verify 7/7 tests passing with overall score â‰¥ 8.5
    status: pending
  - id: phase5-spec-comparison
    content: Compare v1.4 with original spec, document design decisions
    status: pending
  - id: phase5-document-v1_4
    content: Create versions/v1_4_optimized.md with full parameter rationale
    status: pending
  - id: phase6-qdrant
    content: Add Qdrant to docker-compose, implement qdrant_store.py, migrate embeddings
    status: pending
  - id: phase6-judges-package
    content: Create evaluation/judges/ package (client.py, orchestrator.py, aggregator.py, prompt_builder.py, config.json)
    status: pending
  - id: phase6-criteria-package
    content: Create evaluation/criteria/ package with modular criterion definitions (individual folders per criterion)
    status: pending
  - id: phase6-update-runner
    content: Update runner.py to use judges package, remove --with-llm flag (LLM is core)
    status: pending
  - id: phase6-ui-config
    content: Add UI modal for API key configuration (Embeddings + LLM sections)
    status: pending
  - id: phase6-docker-deploy
    content: Create single docker-compose command with API key params
    status: pending
  - id: phase6-ui-polish
    content: Final UI polish - Settings modal, version selector, export reports
    status: pending
  - id: phase7-algorithm-evolution-doc
    content: Update algorithm_evolution/ with v1.4 docs, complete v1.0â†’v1.4 comparison
    status: pending
  - id: phase7-presentation
    content: Create presentation doc with algorithm design, tuning journey, benchmark results
    status: pending
  - id: phase7-future-roadmap
    content: Document next steps (10K episodes, production deployment, CI/CD)
    status: pending
isProject: false
---

# Serafis Recommendation Engine MVP Completion Plan

**Last Updated:** 2026-02-09

---

## Recent Changes (2026-02-09)

### Test Suite Reorganization

- **Deleted:** Old Test 06 (Bookmark Weighting - Mixed Quality)
  - This test used low-quality crypto episodes that correctly failed quality gates
  - The test conflated bookmark weighting with quality gate behavior
  - The algorithm was correctly excluding low-quality content
  - **Rationale:** Quality gates should override engagement signals â€” this is correct behavior matching TikTok/Netflix/Spotify patterns
  
- **Renumbered:** 
  - Old Test 07 â†’ **New Test 06** (Recency Scoring)
  - Old Test 08 â†’ **New Test 07** (Bookmark Weighting - High Quality)

- **Current Test Suite:** 7 tests (01-07)

### Current Test Status (v1.3)

| Test | Name | Status | Score |
|------|------|--------|-------|
| 01 | Cold Start Quality | âœ… | 9.50 |
| 02 | Personalization Differs | âœ… | 8.65 |
| 03 | Quality Gates | âœ… | 10.00 |
| 04 | Excluded Episodes | âœ… | 10.00 |
| 05 | Category Personalization | âœ… | 9.04 |
| 06 | Recency Scoring | âœ… | 7.56 |
| 07 | Bookmark Weighting | âŒ | 7.88 (`llm_hypothesis_alignment` fails at 4.0, threshold 6.0) |

**Goal:** 7/7 tests passing with maximized scores (overall â‰¥ 8.5)

---

## Execution Instructions

1. **Phase 4** â€” Create v1.4, tune to pass all 7 tests
2. **Phase 5** â€” Compare with original spec, document final algorithm
3. **Phase 6** â€” Deployment infrastructure (Qdrant, LiteLLM, Multi-LLM Judge, Docker)
4. **Phase 7** â€” Documentation and presentation
5. **Commit frequently** with descriptive messages

---

## Context Files for Execution

### Algorithm Evolution (critical for Phase 4-5)

- `/Users/rohankatakam/Documents/serafis/rec/algorithm_evolution/README.md`
- `/Users/rohankatakam/Documents/serafis/rec/algorithm_evolution/CHANGELOG.md`
- `/Users/rohankatakam/Documents/serafis/rec/algorithm_evolution/PERFORMANCE_COMPARISON.md`
- `/Users/rohankatakam/Documents/serafis/rec/algorithms/v1_3_tuned/config.json`

### Evaluation Framework

- `/Users/rohankatakam/Documents/serafis/rec/evaluation/test_cases/README.md`
- `/Users/rohankatakam/Documents/serafis/rec/evaluation/test_cases/07_bookmark_weighting.json`

### Infrastructure (for Phase 6)

- `/Users/rohankatakam/Documents/serafis/rec/docker-compose.yml`
- `/Users/rohankatakam/Documents/serafis/rec/server/server.py`
- `/Users/rohankatakam/Documents/serafis/rec/server/qdrant_store.py`

---

## Phase 4: Algorithm Tuning to Maximize All Criteria

### 4.1 Create v1.4 Algorithm

```bash
cp -r algorithms/v1_3_tuned algorithms/v1_4_optimized
```

Update `manifest.json`:
```json
{
  "version": "1.4",
  "name": "v1_4_optimized",
  "description": "Final optimized algorithm with tuned bookmark weighting"
}
```

Update `config.json` with tuned parameters (see 4.2).

### 4.2 Test 07 Tuning Strategy (Bookmark Weighting)

**Current State (v1.3):**
- `bookmark_weight: 5.0`
- 6/10 crypto episodes in bookmark-crypto scenario
- `llm_hypothesis_alignment: 4.0` (need 6.0+)

**Tuning Options (Low â†’ High Friction):**

| Option | Change | Expected Impact | Risk |
|--------|--------|-----------------|------|
| **A. Increase bookmark_weight** | 5.0 â†’ 7.0 or 10.0 | Stronger bookmark signal | Diminishing returns |
| **B. Reduce user_vector_limit** | 10 â†’ 5 | Fewer engagements = less dilution | May hurt other tests |
| **C. Add category_boost** | New param: boost episodes matching bookmarked categories | Direct category influence | Architecture change |
| **D. Engagement recency decay** | Weight recent engagements higher | Recent bookmarks dominate | Complexity |

**Recommended Approach:**
1. First try **Option A** (`bookmark_weight: 7.0`)
2. If not enough, try `bookmark_weight: 10.0`
3. If still failing, experiment with **Option C** (`category_boost` for bookmarked topics)
4. If 7/10+ crypto achieved but LLM still scores low, consider lowering threshold to 5.0

**category_boost Implementation (if needed):**
```python
if candidate_category in user_bookmarked_categories:
    score += category_boost_weight  # e.g., 0.1
```
*Decision: Experiment with this and include in v1.4 only if testing shows improvement.*

### 4.3 Maximize All Criteria Scores

Beyond pass/fail, improve these specific criteria:

| Test | Name | Current Score | Target | Improvable Criteria |
|------|------|---------------|--------|---------------------|
| 01 | Cold Start | 9.50 | 9.7+ | `llm_diversity` (8.0 â†’ 9.0) |
| 02 | Personalization | 8.65 | 8.8+ | `llm_diversity` (5.0 â†’ 7.0) |
| 05 | Category | 9.04 | 9.2+ | `crypto_category_match` (5.5 â†’ 7.0) |
| 06 | Recency | 7.56 | 8.0+ | `recency_score_ordering` (5.74 â†’ 7.0) |
| 07 | Bookmark | 7.88 | 8.5+ | `llm_hypothesis_alignment` (4.0 â†’ 6.0+) |

### 4.4 Run Full Test Suite

```bash
cd rec/evaluation
python runner.py --verbose --save
```

*Note: LLM evaluation runs by default (no `--with-llm` flag needed). At least one LLM API key required.*

**Success Criteria:**
- All 7 tests passing
- Overall score â‰¥ 8.5
- No regressions from v1.3

---

## Phase 5: Spec Comparison & Final Algorithm Selection

### 5.1 Compare v1.4 with Original Spec

Verify v1.4 implements all original requirements:

| Original Requirement | v1.4 Status |
|---------------------|-------------|
| Sum-of-similarities | âœ… `use_sum_similarities: true` |
| Top 10 engagements | âœ… `user_vector_limit: 10` |
| Depth of engagement (bookmarks) | ðŸ”§ Tuned in v1.4 |
| Credibility weighted higher | âœ… `credibility_multiplier: 1.5` |
| Category overlap (v2) | â¸ï¸ Consider `category_boost` if needed for v1.4 |

### 5.2 Document Final Algorithm

Create `algorithm_evolution/versions/v1_4_optimized.md`:
- All parameter values with rationale
- Performance comparison (v1.0 â†’ v1.4)
- Design decisions and trade-offs
- What worked, what didn't

---

## Phase 6: Deployment & Infrastructure (Expanded)

### 6.1 Qdrant Integration

| Task | Description |
|------|-------------|
| Add Qdrant to docker-compose | `qdrant/qdrant:latest` container |
| Create `server/qdrant_store.py` | Collection management, upsert, search |
| Migrate embeddings | Load from JSON cache â†’ Qdrant on startup |
| Update recommendation engine | Use Qdrant for similarity search |
| Fallback mode | If Qdrant unavailable, use JSON (for dev) |

**Docker Compose Addition:**
```yaml
qdrant:
  image: qdrant/qdrant:latest
  ports:
    - "6333:6333"
  volumes:
    - qdrant_storage:/qdrant/storage
```

### 6.2 Modular Evaluation Architecture

**Design Principle:** LLM evaluation is core (not optional). No `--with-llm` flag.

| Package | Purpose |
|---------|---------|
| `evaluation/judges/` | Multi-LLM orchestration, LiteLLM client, aggregation |
| `evaluation/criteria/` | Individual criterion definitions (LLM and deterministic) |

**New Directory Structure:**
```
evaluation/
â”œâ”€â”€ judges/                    # LLM evaluation package
â”‚   â”œâ”€â”€ client.py              # LiteLLM async wrapper
â”‚   â”œâ”€â”€ orchestrator.py        # Multi-LLM coordination
â”‚   â”œâ”€â”€ aggregator.py          # Two-stage aggregation
â”‚   â”œâ”€â”€ prompt_builder.py      # Dynamic prompts from criteria
â”‚   â””â”€â”€ config.json            # Model settings
â”‚
â”œâ”€â”€ criteria/                  # Modular criterion definitions
â”‚   â”œâ”€â”€ relevance/definition.json + README.md
â”‚   â”œâ”€â”€ diversity/definition.json + README.md
â”‚   â”œâ”€â”€ quality/definition.json + README.md
â”‚   â”œâ”€â”€ hypothesis_alignment/definition.json + README.md
â”‚   â”œâ”€â”€ avg_credibility/definition.json + logic.py + README.md  (deterministic)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ deprecated/                # Old code (reference only)
â”‚   â””â”€â”€ llm_judge.py
```

**Criterion Types:**
- **LLM Criteria**: Prompt-based evaluation (relevance, diversity, quality, etc.)
- **Deterministic Criteria**: Python functions (avg_credibility, crypto_count, etc.)

**Scaling:** Add new criteria/tests/providers without code changes.

### 6.3 Multi-LLM Judge System (MVP Feature)

**Problem Solved:** Test 01 has ~50% pass rate due to LLM evaluation variability.

> **Detailed Implementation Plan:** See [multi-llm_judge_infrastructure_474849c6.plan.md](plans/multi-llm_judge_infrastructure_474849c6.plan.md) for complete specification.

**Key Features:**
- LiteLLM unified interface for OpenAI, Gemini, Anthropic
- Per-criterion LLM calls (maximum modularity, better debugging)
- Two-stage aggregation (within-model mean â†’ cross-model mean)
- Configurable N samples per judge (default: 3)
- Temperature 0.8 (research-backed for better calibration)
- Graceful degradation if one LLM provider fails
- Consensus metrics and uncertainty reporting

**LiteLLM Model Mapping:**

| Provider | Embeddings | LLM Judge |
|----------|------------|-----------|
| OpenAI | `text-embedding-3-small` | `gpt-4o-mini` |
| Gemini | N/A | `gemini/gemini-2.0-flash` |
| Anthropic | N/A | `claude-3-5-sonnet-20241022` |

**Cost/Time Estimates:**
- ~126 API calls per full run (7 tests Ã— 3 criteria Ã— 2 judges Ã— 3 samples)
- Cost: ~$0.10-0.20 per full run
- Time: 60-120 seconds with async parallelization

### 6.4 Docker Deployment

**CLI Parameters:**
```bash
docker-compose up --build \
  -e OPENAI_API_KEY=sk-... \         # Required (embeddings + LLM)
  -e GEMINI_API_KEY=... \            # Optional (LLM judge)
  -e ANTHROPIC_API_KEY=sk-ant-... \  # Optional (LLM judge)
  -e DATA_PATH=/data/episodes.json   # Dataset location
```

**UI Configuration (if no keys in CLI):**

On first launch, show modal with:

1. **Embeddings Section**
   - OpenAI API Key (required)

2. **LLM Section**
   - OpenAI API Key (use same as embeddings)
   - Add New Provider dropdown:
     - Gemini API Key
     - Anthropic API Key
   - Checkbox to enable each as LLM judge

3. **Dataset Section**
   - Folder path selector (local filesystem)

### 6.5 Data Handling

**Approach:** Simple folder path (user downloads from S3 manually)

| Task | Description |
|------|-------------|
| S3 Bucket | Host `eval_909_feb2026.json` in private S3 |
| Instructions | README with S3 download instructions |
| UI Path Selector | Browse for local folder containing dataset |
| Validation | Verify dataset schema on load |

### 6.6 Final UI Polish

- Settings modal refinements
- Algorithm/dataset version selector UX
- Export reports to JSON/CSV
- Loading states and error handling
- Visual consistency pass

---

## Phase 7: Documentation & Presentation

### 7.1 Algorithm Evolution Document

Update `algorithm_evolution/` with:
- v1.4 documentation (`versions/v1_4_optimized.md`)
- Complete performance comparison (v1.0 â†’ v1.4)
- Final tuning decisions with rationale

### 7.2 Presentation Content

| Section | Content |
|---------|---------|
| Overview | What is "For You" recommendation engine |
| Algorithm Design | v1.4 architecture, scoring formula, quality gates |
| Tuning Journey | v1.0 â†’ v1.2 â†’ v1.3 â†’ v1.4 with learnings |
| Test Framework | 7 tests, profiles, multi-LLM judge |
| Benchmark Results | All 7 tests passing, score breakdown |
| Infrastructure | Docker, Qdrant, LiteLLM integration |
| Next Steps | 10K episodes, production deployment, CI/CD |

### 7.3 Future Roadmap

- Scale to 10K episodes (Qdrant ready)
- Production deployment architecture
- CI/CD pipeline for algorithm testing
- User feedback integration
- A/B testing framework

---

## Implementation Order

| Step | Phase | Task | Priority |
|------|-------|------|----------|
| 1 | 4 | Clone v1_3 â†’ v1_4, update manifest | High |
| 2 | 4 | Tune v1.4 with `bookmark_weight: 7.0`, run tests | High |
| 3 | 4 | If needed, try `bookmark_weight: 10.0` | High |
| 4 | 4 | If still failing, experiment with `category_boost` | Medium |
| 5 | 4 | Run full test suite, verify 7/7 passing | High |
| 6 | 4 | Maximize all criteria scores | Medium |
| 7 | 5 | Document v1.4, compare with original spec | Medium |
| 8 | 6 | Add Qdrant to docker-compose | High |
| 9 | 6 | Create `evaluation/judges/` package (LiteLLM + orchestrator) | High |
| 10 | 6 | Create `evaluation/criteria/` package (modular criterion defs) | High |
| 11 | 6 | Update runner.py (remove --with-llm, use judges package) | High |
| 12 | 6 | Add UI model configuration modal | Medium |
| 13 | 6 | Test end-to-end Docker deployment | High |
| 14 | 7 | Update algorithm_evolution docs | Medium |
| 15 | 7 | Create final presentation | Medium |

---

## How Professional Recommendation Engines Handle Bookmarks

The algorithm correctly follows industry patterns:

| Platform | Bookmark Action | Reappears in Feed? | Signal Usage |
|----------|----------------|-------------------|--------------|
| **TikTok** | Favorite/Save | No | Strong signal for similar content |
| **Netflix** | My List | No | Boosts similar genres/actors |
| **Spotify** | Like Song | No | Influences Discover Weekly |
| **Serafis** | Bookmark | No â€” excluded via `excluded_ids` | Strong signal for user vector |

**Key Behaviors (verified in v1.3):**
1. âœ… Bookmarked episodes excluded from new recommendations
2. âœ… Bookmark signal influences user vector (5x weight in v1.3)
3. âœ… Quality gates apply to ALL new recommendations
4. âœ… Low-quality bookmarked content still influences vector, but new recs are filtered

---

## Technology Stack Summary

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Vector DB** | Qdrant | Embedding storage, similarity search at scale |
| **Backend** | FastAPI (Python) | API server, algorithm execution |
| **Frontend** | React + Vite | Development/demo UI |
| **Embeddings** | OpenAI text-embedding-3-small | 1536-dim episode embeddings |
| **LLM Judge** | LiteLLM (OpenAI/Gemini/Anthropic) | Multi-provider qualitative evaluation |
| **Deployment** | Docker Compose | Single-command deployment |

---

## Key Files Reference

| Purpose | Path |
|---------|------|
| Algorithm v1.3 | `algorithms/v1_3_tuned/` |
| Algorithm v1.4 (to create) | `algorithms/v1_4_optimized/` |
| Test Runner | `evaluation/runner.py` |
| Test Cases | `evaluation/test_cases/*.json` (7 tests) |
| Judges Package (to create) | `evaluation/judges/` |
| Criteria Package (to create) | `evaluation/criteria/` |
| Algorithm Evolution | `algorithm_evolution/` |
| Docker Config | `docker-compose.yml` |
| Server | `server/server.py` |
| Qdrant Store | `server/qdrant_store.py` |
| Frontend | `prototype/src/` |
